import os
import tempfile
import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from dotenv import load_dotenv

# Carrega as vari√°veis do arquivo .env
load_dotenv()

# Obt√©m a chave da API do ambiente
chave_api = os.getenv('OPENAI_API_KEY')

def carregar_documentos_pdf(lista_arquivos):
    """Carrega o conte√∫do de m√∫ltiplos arquivos PDF."""
    textos_extraidos = [] # Lista para armazenar os textos extra√≠dos de cada PDF

    # Itera sobre cada arquivo enviado
    for arquivo_pdf in lista_arquivos:
        # Cria um arquivo tempor√°rio para salvar o conte√∫do do PDF
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as arquivo_temporario:
            # Escreve o conte√∫do do objeto BytesIO no arquivo tempor√°rio
            arquivo_temporario.write(arquivo_pdf.read())
            caminho_arquivo = arquivo_temporario.name # Obt√©m o caminho do arquivo tempor√°rio

        # Carrega o PDF utilizando o PyPDFLoader com o caminho do arquivo tempor√°rio
        carregador_pdf = PyPDFLoader(caminho_arquivo)
        documentos_carregados = carregador_pdf.load() # Retorna uma lista de documentos com atributo 'page_content'

        # Itera sobre cada documento carregado e armazena seu conte√∫do
        for documento in documentos_carregados:
            textos_extraidos.append(documento.page_content)

        # Remove o arquivo tempor√°rio ap√≥s o processamento
        os.remove(caminho_arquivo)

    # Retorna o conte√∫do concatenado de todos os PDFs
    return '\n'.join(textos_extraidos)

def main():
    """Fun√ß√£o principal para configurar e executar a interface da aplica√ß√£o Streamlit."""
    # Inicializa a mem√≥ria de conversa no session_state, se ainda n√£o existir
    if 'memoria' not in st.session_state:
        # Cria uma inst√¢ncia de ConversationBufferMemory para armazenar o hist√≥rico do chat
        st.session_state.memoria = ConversationBufferMemory()

    # Inicializa a chain no session_state, se ainda n√£o existir
    if 'chain' not in st.session_state:
        st.session_state.chain = None

    # Configura o t√≠tulo e o √≠cone da p√°gina no Streamlit
    st.set_page_config(page_title='Chat com arquivos PDF')
    # Exibe o t√≠tulo principal da aplica√ß√£o
    st.title('üí¨ Chat com arquivos PDF')

    # Configura a barra lateral para o upload de documentos
    with st.sidebar:
        # Exibe o cabe√ßalho da se√ß√£o de upload de documentos
        st.header('üìÅ Upload de Documentos')
        # Permite o envio de m√∫ltiplos arquivos PDF com uma √°rea de upload
        arquivos_pdfs = st.file_uploader(
            'Selecione os arquivos PDF', # Texto exibido na √°rea de upload
            type='pdf', # Limita o upload a arquivos com extens√£o .pdf
            accept_multiple_files=True, # Permite o envio de m√∫ltiplos arquivos
            help='Voc√™ pode fazer upload de m√∫ltiplos arquivos PDF' # Ajuda exibida ao usu√°rio
        )

        # Verifica se o usu√°rio enviou arquivos PDF
        if arquivos_pdfs:
            # Exibe um bot√£o para processar os PDFs enviados
            if st.button('Processar PDFs', use_container_width=True):
                # Exibe um spinner indicando que o processamento est√° em andamento
                with st.spinner('Processando documentos...'):
                    # Processa os arquivos PDF e extrai o conte√∫do textual
                    documento = carregar_documentos_pdf(arquivos_pdfs)

                    # Escapa as chaves '{' e '}' para evitar erros no .format()
                    # Isso substitui todas as ocorr√™ncias de '{' por '{{' e '}' por '}}
                    documento_escapado = documento.replace('{', '{{').replace('}', '}}')

                    # Cria o prompt de sistema utilizando o conte√∫do extra√≠do dos PDFs
                    prompt_sistema = """
                    Voc√™ √© um assistente amig√°vel com acesso √†s informa√ß√µes contidas no documento abaixo:

                    ####
                    {}
                    ####

                    Sua tarefa √© responder √†s perguntas baseando-se exclusivamente nas informa√ß√µes apresentadas acima e no hist√≥rico da conversa.
                    Se a resposta n√£o puder ser determinada com esses dados, responda que n√£o sabe, sem adivinhar ou inventar informa√ß√µes.
                    """.format(documento_escapado)

                    # Cria o template de prompt para o chat
                    template = ChatPromptTemplate.from_messages([
                        ('system', prompt_sistema),
                        ('placeholder', '{chat_history}'),
                        ('user', '{input}')
                    ])

                    # Inicializa o modelo e a chain
                    modelo_chat = ChatOpenAI(model='gpt-4o', api_key=chave_api)
                    st.session_state.chain = template | modelo_chat

    if st.session_state.chain is not None:
        # Exibir as mensagens anteriores armazenadas na mem√≥ria
        # A propriedade buffer_as_messages retorna uma lista de objetos de mensagem com atributos 'type' e 'content'
        for mensagem in st.session_state.memoria.buffer_as_messages:
            # Cria uma mensagem no chat de acordo com o tipo ('human' para usu√°rio ou 'ai' para assistente)
            with st.chat_message(mensagem.type):
                # Exibe o conte√∫do da mensagem
                st.write(mensagem.content)

        # Caixa de entrada para o usu√°rio inserir sua mensagem
        mensagem_usuario = st.chat_input('Digite sua mensagem...')

        if mensagem_usuario:
            # Exibe a mensagem do usu√°rio
            with st.chat_message('human'):
                st.write(mensagem_usuario)

            # Exibe a resposta do assistente usando streaming
            with st.chat_message('ai'):
                placeholder_resposta = st.empty()
                placeholder_resposta.write('Pensando...')

                # Usa chain.stream() com a entrada e o hist√≥rico do chat
                resposta = placeholder_resposta.write_stream(
                    st.session_state.chain.stream({
                        'input': mensagem_usuario,
                        'chat_history': st.session_state.memoria.buffer_as_messages # Cont√©m todas as mensagens trocadas at√© o momento, tanto do usu√°rio quanto da IA.
                    })
                )

            # Adiciona a mensagem do usu√°rio ao hist√≥rico antes de gerar resposta
            st.session_state.memoria.chat_memory.add_user_message(mensagem_usuario)

            # Adiciona resposta do modelo ao hist√≥rico
            st.session_state.memoria.chat_memory.add_ai_message(resposta)

# Verifica se o script est√° sendo executado diretamente e, em caso afirmativo, chama a fun√ß√£o main()
if __name__ == '__main__':
    main()
